name: Weekly Benchmarks

on:
  schedule:
    - cron: '0 7 * * 0' # 07:00 UTC every Sunday
  workflow_dispatch:

permissions:
  contents: read

jobs:
  check-commits:
    name: Check for new commits
    runs-on: ubuntu-latest
    outputs:
      has_new_commits: ${{ steps.check.outputs.has_new_commits }}
    steps:
      - name: Check if there are new commits in last 7 days
        id: check
        shell: bash
        run: |
          # Get commits from last 7 days on main branch
          COMMITS=$(gh api \
            -H "Accept: application/vnd.github+json" \
            -H "X-GitHub-Api-Version: 2022-11-28" \
            "/repos/${{ github.repository }}/commits?sha=main&since=$(date -u -d '7 days ago' '+%Y-%m-%dT%H:%M:%SZ')&per_page=1" \
            --jq 'length')

          if [ "$COMMITS" -gt 0 ]; then
            echo "has_new_commits=true" >> $GITHUB_OUTPUT
            echo "Found $COMMITS new commit(s) in last 7 days"
          else
            echo "has_new_commits=false" >> $GITHUB_OUTPUT
            echo "No new commits in last 7 days, skipping benchmarks"
          fi
        env:
          GH_TOKEN: ${{ github.token }}

  nightly-linux:
    needs: check-commits
    if: needs.check-commits.outputs.has_new_commits == 'true' || github.event_name == 'workflow_dispatch'
    name: Benchmarks (Linux x86_64)
    runs-on: ubuntu-24.04
    timeout-minutes: 90
    env:
      BUILD_DIR: builddir
      # Opt-in switch to run the daemon warm-latency bench (off by default)
      RUN_DAEMON_WARM_BENCH: 'false'
    steps:
      - name: Checkout
        uses: actions/checkout@v5
        with:
          fetch-depth: 0
          submodules: false

      - name: Initialize required submodules
        shell: bash
        run: |
          # Force HTTPS for GitHub to handle SSH submodule URLs in CI
          git config --global url."https://github.com/".insteadOf "git@github.com:"
          git submodule update --init --depth 1 third_party/sqlite-vec-cpp

      - name: Install system dependencies
        shell: bash
        run: |
          set -euo pipefail
          sudo apt-get update
          sudo apt-get install -y --no-install-recommends pkg-config meson ninja-build cmake python3-pip libboost-all-dev

      - name: Install Conan
        shell: bash
        run: |
          python3 -m pip install --user --upgrade pipx || true
          python3 -m pipx ensurepath || true
          pipx install --python python3 conan || pipx install conan
          echo "$HOME/.local/bin" >> "$GITHUB_PATH"
          conan --version
          
          # Detect Conan profile (required for setup.sh)
          conan profile detect --force || true

      - name: Cache Conan
        uses: actions/cache@v4
        with:
          path: |
            ~/.conan2/p
            ~/.conan2/r
            ~/.conan2/recipes
            ~/.conan2/metadata
            ~/.conan2/s
          key: nightly-conan2-linux-${{ hashFiles('conanfile.py', 'meson.build', 'meson_options.txt') }}
          restore-keys: |
            nightly-conan2-linux-

      - name: Configure (Release) and Build
        shell: bash
        run: |
          set -euo pipefail
          rm -rf "$BUILD_DIR"
          
          # Keep AF_UNIX socket path short
          export XDG_RUNTIME_DIR="/run/user/$(id -u)"
          mkdir -p "$XDG_RUNTIME_DIR" || true
          
          # Use setup.sh with CI environment variables for consistent configuration
          export YAMS_CONAN_HOST_PROFILE="./conan/profiles/host-linux-clang"
          export YAMS_CONAN_ARCH="x86_64"
          export YAMS_EXTRA_MESON_FLAGS="-Dbuild-tests=true -Denable-onnx=disabled"
          
          ./setup.sh Release
          meson compile -C builddir

      - name: Run Bench Test Suite
        shell: bash
        run: |
          set -euo pipefail
          meson test -C "$BUILD_DIR" --suite bench -t 5 --no-rebuild

      - name: Run Daemon Warm-Latency Bench (opt-in)
        if: env.RUN_DAEMON_WARM_BENCH == 'true'
        shell: bash
        env:
          YAMS_ENABLE_DAEMON_BENCH: '1'
        run: |
          set -euo pipefail
          echo "Running bench_daemon_warm_latency (opt-in)..."
          meson test -C "$BUILD_DIR" bench_daemon_warm_latency --suite bench --no-rebuild -t 3

      - name: Run Bus Micro-benchmark
        id: bench
        shell: bash
        run: |
          set -euo pipefail
          mkdir -p "$BUILD_DIR/bench_results"
          echo "Running yams_bus_bench (SPSC default)..."
          RAW=$("$BUILD_DIR"/src/benchmarks/yams_bus_bench)
          echo "$RAW"
          # Convert key=value pairs to JSON using bash
          ts=$(date -Is)
          json='{"timestamp":"'"$ts"'","bench":{'
          first=1
          for kv in $RAW; do
            case "$kv" in
              *=*)
                key="${kv%%=*}"
                val="${kv#*=}"
                if [ $first -eq 0 ]; then json="$json,"; else first=0; fi
                json="$json\"$key\":\"$val\""
                ;;
            esac
          done
          json="$json}}"
          printf '%s\n' "$json" > "$BUILD_DIR/bench_results/bus_bench.json"
          echo "Wrote bench_results/bus_bench.json"

      - name: Run API Benchmarks
        shell: bash
        run: |
          set -euo pipefail
          echo "Running yams_api_benchmarks..."
          ( cd "$BUILD_DIR" && ./tests/benchmarks/yams_api_benchmarks ) || exit 1

      - name: Run Search Benchmarks
        shell: bash
        run: |
          set -euo pipefail
          echo "Running yams_search_benchmarks..."
          ( cd "$BUILD_DIR" && ./tests/benchmarks/yams_search_benchmarks ) || exit 1

      - name: Run Tree Diff Benchmarks (PBI-043)
        shell: bash
        run: |
          set -euo pipefail
          echo "Running tree_diff_benchmarks..."
          ( cd "$BUILD_DIR" && ./tests/benchmarks/tree_diff_benchmarks ) || exit 1

      - name: Regression Check (API)
        shell: bash
        run: |
          set -euo pipefail
          python3 tests/scripts/check_regression.py \
            "$BUILD_DIR/bench_results/api_benchmarks.json" \
            tests/benchmarks/baseline/api_benchmarks.baseline.json \
            --threshold 0.5 \
            --output "$BUILD_DIR/bench_results/api_benchmarks_regression.json" \
            --fail-on-regression

      - name: Regression Check (Search)
        shell: bash
        run: |
          set -euo pipefail
          python3 tests/scripts/check_regression.py \
            "$BUILD_DIR/bench_results/search_benchmarks.json" \
            tests/benchmarks/baseline/search_benchmarks.baseline.json \
            --threshold 0.5 \
            --output "$BUILD_DIR/bench_results/search_benchmarks_regression.json" \
            --fail-on-regression

      - name: Regression Check (Tree Diff - PBI-043)
        shell: bash
        run: |
          set -euo pipefail
          python3 tests/scripts/check_regression.py \
            "$BUILD_DIR/bench_results/tree_diff_benchmark_results.json" \
            tests/benchmarks/baseline/tree_diff_benchmarks.baseline.json \
            --threshold 0.5 \
            --output "$BUILD_DIR/bench_results/tree_diff_benchmarks_regression.json" \
            --fail-on-regression

      - name: Summarize Benchmarks in Job Summary
        if: always()
        shell: bash
        run: |
          set -euo pipefail
          SUMMARY_FILE="$GITHUB_STEP_SUMMARY"
          echo "### Bench Results" >> "$SUMMARY_FILE"
          if [ -d "$BUILD_DIR/bench_results" ] && ls "$BUILD_DIR/bench_results"/*.json >/dev/null 2>&1; then
            for json in "$BUILD_DIR/bench_results"/*.json; do
              echo >> "$SUMMARY_FILE"
              echo "#### $(basename "$json")" >> "$SUMMARY_FILE"
              echo '```json' >> "$SUMMARY_FILE"
              sed -e 's/[[:cntrl:]]//g' "$json" | head -c 120000 >> "$SUMMARY_FILE" || true
              echo >> "$SUMMARY_FILE"
              echo '```' >> "$SUMMARY_FILE"
            done
          else
            echo "_No bench_results found._" >> "$SUMMARY_FILE"
          fi

      - name: Upload Benchmark Artifacts
        uses: actions/upload-artifact@v4
        with:
          name: nightly-benchmarks-${{ github.run_number }}
          path: |
            ${{ env.BUILD_DIR }}/bench_results/bus_bench.json
            ${{ env.BUILD_DIR }}/bench_results/api_benchmarks.json
            ${{ env.BUILD_DIR }}/bench_results/api_benchmark_report.json
            ${{ env.BUILD_DIR }}/bench_results/api_benchmark_report.md
            ${{ env.BUILD_DIR }}/bench_results/search_benchmarks.json
            ${{ env.BUILD_DIR }}/bench_results/search_benchmark_report.json
            ${{ env.BUILD_DIR }}/bench_results/search_benchmark_report.md
            ${{ env.BUILD_DIR }}/bench_results/api_benchmarks_regression.json
            ${{ env.BUILD_DIR }}/bench_results/search_benchmarks_regression.json
            ${{ env.BUILD_DIR }}/bench_results/tree_diff_benchmark_results.json
            ${{ env.BUILD_DIR }}/bench_results/tree_diff_benchmarks_regression.json
          retention-days: 7
